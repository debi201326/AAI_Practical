{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmbZjHSGEGSutnSFHsoKn9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debi201326/AAI_Practical/blob/main/AAI_prac_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical 6: Choose a specific application domain (robotics or game playing) and design a reinforcement learning system.\n",
        "(Tic-Tac-Toe)\n"
      ],
      "metadata": {
        "id": "fZUsv043Wriq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "WRWSr3vlVNzf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.board = [' '] * 9\n",
        "        self.current_player = 'X'  # Human is X, AI is O\n",
        "        self.game_over = False\n",
        "        self.winner = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = [' '] * 9\n",
        "        self.current_player = 'X'\n",
        "        self.game_over = False\n",
        "        self.winner = None\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        # Convert the board to a tuple for use as a dictionary key\n",
        "        return tuple(self.board)\n",
        "\n",
        "    def available_moves(self):\n",
        "        return [i for i, spot in enumerate(self.board) if spot == ' ']\n",
        "\n",
        "    def make_move(self, position):\n",
        "        if self.board[position] == ' ' and not self.game_over:\n",
        "            self.board[position] = self.current_player\n",
        "            self.check_game_over()\n",
        "            if not self.game_over:\n",
        "                self.current_player = 'O' if self.current_player == 'X' else 'X'\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def check_game_over(self):\n",
        "        # Check rows\n",
        "        for i in range(0, 9, 3):\n",
        "            if self.board[i] == self.board[i+1] == self.board[i+2] != ' ':\n",
        "                self.game_over = True\n",
        "                self.winner = self.board[i]\n",
        "                return\n",
        "        # Check columns\n",
        "        for i in range(3):\n",
        "            if self.board[i] == self.board[i+3] == self.board[i+6] != ' ':\n",
        "                self.game_over = True\n",
        "                self.winner = self.board[i]\n",
        "                return\n",
        "        # Check diagonals\n",
        "        if self.board[0] == self.board[4] == self.board[8] != ' ':\n",
        "            self.game_over = True\n",
        "            self.winner = self.board[0]\n",
        "            return\n",
        "        if self.board[2] == self.board[4] == self.board[6] != ' ':\n",
        "            self.game_over = True\n",
        "            self.winner = self.board[2]\n",
        "            return\n",
        "        # Check for tie\n",
        "        if ' ' not in self.board:\n",
        "            self.game_over = True\n",
        "            self.winner = None\n",
        "\n",
        "    def print_board(self):\n",
        "        print(\"-------------\")\n",
        "        for i in range(3):\n",
        "            print(f\"| {self.board[i*3]} | {self.board[i*3+1]} | {self.board[i*3+2]} |\")\n",
        "            print(\"-------------\")\n",
        "\n",
        "    def play_human_move(self):\n",
        "        available = self.available_moves()\n",
        "        print(\"Available positions:\", available)\n",
        "        while True:\n",
        "            try:\n",
        "                pos = int(input(\"Enter your move (0-8): \"))\n",
        "                if pos in available:\n",
        "                    self.make_move(pos)\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"Invalid move. Try again.\")\n",
        "            except ValueError:\n",
        "                print(\"Please enter a number between 0-8.\")\n"
      ],
      "metadata": {
        "id": "I8lZbr59XKay"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self):\n",
        "        self.q_table = {}\n",
        "        self.learning_rate = 0.1\n",
        "        self.discount_factor = 0.9\n",
        "        self.epsilon = 0.3  # Exploration rate\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.min_epsilon = 0.01\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        if state not in self.q_table:\n",
        "            self.q_table[state] = np.zeros(9)\n",
        "        return self.q_table[state][action]\n",
        "\n",
        "    def choose_action(self, state, available_actions):\n",
        "        if random.random() < self.epsilon:\n",
        "            # Exploration: random action\n",
        "            return random.choice(available_actions)\n",
        "        else:\n",
        "            # Exploitation: best known action\n",
        "            q_values = [self.get_q_value(state, a) for a in available_actions]\n",
        "            max_q = max(q_values)\n",
        "            # If multiple actions have the same max Q value, choose randomly among them\n",
        "            best_actions = [a for a, q in zip(available_actions, q_values) if q == max_q]\n",
        "            return random.choice(best_actions)\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        if state not in self.q_table:\n",
        "            self.q_table[state] = np.zeros(9)\n",
        "\n",
        "        current_q = self.q_table[state][action]\n",
        "\n",
        "        if done:\n",
        "            max_next_q = 0\n",
        "        else:\n",
        "            if next_state not in self.q_table:\n",
        "                self.q_table[next_state] = np.zeros(9)\n",
        "            max_next_q = max(self.q_table[next_state])\n",
        "\n",
        "        # Q-learning update rule\n",
        "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n",
        "        self.q_table[state][action] = new_q\n",
        "\n",
        "        # Decay epsilon\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n"
      ],
      "metadata": {
        "id": "0cwzRtxT2uUK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent(episodes=10000):\n",
        "    env = TicTacToe()\n",
        "    agent = QLearningAgent()\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # AI's turn (O)\n",
        "            available_actions = env.available_moves()\n",
        "            action = agent.choose_action(state, available_actions)\n",
        "            env.make_move(action)\n",
        "            next_state = env.get_state()\n",
        "\n",
        "            # Check if game ended with AI's move\n",
        "            if env.game_over:\n",
        "                if env.winner == 'O':\n",
        "                    reward = 1  # Win\n",
        "                elif env.winner == 'X':\n",
        "                    reward = -1  # Loss (shouldn't happen in training as human isn't playing)\n",
        "                else:\n",
        "                    reward = 0.5  # Tie\n",
        "                done = True\n",
        "            else:\n",
        "                reward = 0\n",
        "                # Human's turn (simulated random moves during training)\n",
        "                available_actions = env.available_moves()\n",
        "                if available_actions:  # If game isn't over\n",
        "                    random_action = random.choice(available_actions)\n",
        "                    env.make_move(random_action)\n",
        "                    if env.game_over:\n",
        "                        if env.winner == 'X':\n",
        "                            reward = -1  # Loss\n",
        "                        elif env.winner == 'O':\n",
        "                            reward = 1  # Win (shouldn't happen here)\n",
        "                        else:\n",
        "                            reward = 0.5  # Tie\n",
        "                        done = True\n",
        "\n",
        "            # Learn from the experience\n",
        "            agent.learn(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "\n",
        "        if (episode + 1) % 1000 == 0:\n",
        "            print(f\"Episode {episode + 1}/{episodes}, Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    return agent\n"
      ],
      "metadata": {
        "id": "yKpMrsjn2uWV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_game(agent):\n",
        "    env = TicTacToe()\n",
        "    state = env.reset()\n",
        "    env.print_board()\n",
        "\n",
        "    while not env.game_over:\n",
        "        if env.current_player == 'X':  # Human's turn\n",
        "            env.play_human_move()\n",
        "            print(\"\\nHuman's move:\")\n",
        "            env.print_board()\n",
        "        else:  # AI's turn\n",
        "            available_actions = env.available_moves()\n",
        "            action = agent.choose_action(env.get_state(), available_actions)\n",
        "            env.make_move(action)\n",
        "            print(\"\\nAI's move:\")\n",
        "            env.print_board()\n",
        "\n",
        "        if env.game_over:\n",
        "            if env.winner == 'X':\n",
        "                print(\"You win!\")\n",
        "            elif env.winner == 'O':\n",
        "                print(\"AI wins!\")\n",
        "            else:\n",
        "                print(\"It's a tie!\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "N5UyN8LZ2uYo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the agent\n",
        "print(\"Training AI agent...\")\n",
        "ai_agent = train_agent(episodes=10000)\n",
        "\n",
        "# Play against the AI\n",
        "print(\"\\nTraining complete! Let's play!\")\n",
        "while True:\n",
        "    play_game(ai_agent)\n",
        "    again = input(\"Play again? (y/n): \").lower()\n",
        "    if again != 'y':\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dFuNkt92uat",
        "outputId": "a682d947-6eac-4882-a3aa-1b7ba8f915c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training AI agent...\n",
            "Episode 1000/10000, Epsilon: 0.010\n",
            "Episode 2000/10000, Epsilon: 0.010\n",
            "Episode 3000/10000, Epsilon: 0.010\n",
            "Episode 4000/10000, Epsilon: 0.010\n",
            "Episode 5000/10000, Epsilon: 0.010\n",
            "Episode 6000/10000, Epsilon: 0.010\n",
            "Episode 7000/10000, Epsilon: 0.010\n",
            "Episode 8000/10000, Epsilon: 0.010\n",
            "Episode 9000/10000, Epsilon: 0.010\n",
            "Episode 10000/10000, Epsilon: 0.010\n",
            "\n",
            "Training complete! Let's play!\n",
            "-------------\n",
            "|   |   |   |\n",
            "-------------\n",
            "|   |   |   |\n",
            "-------------\n",
            "|   |   |   |\n",
            "-------------\n",
            "Available positions: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
            "Enter your move (0-8): 0\n",
            "\n",
            "Human's move:\n",
            "-------------\n",
            "| X |   |   |\n",
            "-------------\n",
            "|   |   |   |\n",
            "-------------\n",
            "|   |   |   |\n",
            "-------------\n",
            "\n",
            "AI's move:\n",
            "-------------\n",
            "| X |   |   |\n",
            "-------------\n",
            "|   |   | O |\n",
            "-------------\n",
            "|   |   |   |\n",
            "-------------\n",
            "Available positions: [1, 2, 3, 4, 6, 7, 8]\n",
            "Enter your move (0-8): 4\n",
            "\n",
            "Human's move:\n",
            "-------------\n",
            "| X |   |   |\n",
            "-------------\n",
            "|   | X | O |\n",
            "-------------\n",
            "|   |   |   |\n",
            "-------------\n",
            "\n",
            "AI's move:\n",
            "-------------\n",
            "| X |   | O |\n",
            "-------------\n",
            "|   | X | O |\n",
            "-------------\n",
            "|   |   |   |\n",
            "-------------\n",
            "Available positions: [1, 3, 6, 7, 8]\n",
            "Enter your move (0-8): 8\n",
            "\n",
            "Human's move:\n",
            "-------------\n",
            "| X |   | O |\n",
            "-------------\n",
            "|   | X | O |\n",
            "-------------\n",
            "|   |   | X |\n",
            "-------------\n",
            "You win!\n",
            "Play again? (y/n): n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cSXXELIkVTwc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}